{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling With Constellate using LDA and NMF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paul King\n",
    "\n",
    "View in GitHub: [Topic Modelling with Constellate](https://github.com/paulkdata/topic_modelling_with_constellate)\n",
    "\n",
    "Or, [Launch in Constellate](https://constellate.org/notebook/own/?repo=https%3A%2F%2Fgithub.com%2Fpaulkdata%2Ftopic_modelling_with_constellate&urlpath=tree%2Ftopic_modelling_with_constellate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This Jupyter notebook-based tutorial describes how to perform topic modelling with the LDA and NMF algorithms on a corpus, and how to quantitatively and qualitatively evaluate them. This tutorial can be run through the Constellate website, so only an internet connection and a JSTOR account linked to your institution is required. If you have a Jupyter notebook/Python environment, you may run this tutorial on your local host, however a JSTOR account is required to build your own dataset. Make sure to download the Constellate Client library if you are running this tutorial locally: [Consellate Client](https://constellate.org/docs/constellate-client/)\n",
    "\n",
    "## Target Audience\n",
    "This tutorial is suitable both for independent learners interested in text analytics and digital humanities, as well as in a classroom or workshop with an instructor. Learners should have some base understanding of using Python and Jupyter notebooks, as well as knowledge of basic text analytics and interest in learning more. If you are not familiar with Python or text analytics such as tokenizing text and using regular expressions, Constellate has a wide variety of tutorials available here: [Constellate Tutorials](https://constellate.org/tutorials)\n",
    "\n",
    "## Learning Outcomes\n",
    "Upon completion of this tutorial, learners will be able to:\n",
    "- Download and convert text data into a format understandable by topic models\n",
    "- Run LDA and NMF topic models using the Gensim library\n",
    "- Quantitatively evaluate topic models on computational time and coherence score\n",
    "- Qualitatively evaluate topic models based on resulting topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling with Constellate\n",
    "Topic modelling is a statistical text mining technique. Topic modelling is used to extract \"topics\", representative sets of words, from a corpus of texts. The most common algorithm for topic modelling is called Latent Dirichlet Allocation, or LDA. You can use LDA in Constellate, however, because Constellate uses cloud computing, topic modelling with LDA can be very slow. In such cases, it can be quicker to use an algorithm called Non-Negative Matrix Factorization, or NMF. In this tutorial, we will import a corpus, pre-process unigram tokens, and perform topic modelling with LDA and NMF and compare them by time, coherence score, and resultant topics.\n",
    "\n",
    "Before doing this tutorial, I recommend that you are familiar with the basics of Constellate and using Python in Jupyter notebooks. It will also help to be familiar with basic text analytics including pre-processing and tokenizing texts, however this code will be supplied in the tutorial. Constellate has many additional tutorials on their website for text analytics.\n",
    "\n",
    "Both LDA and NMF are available through the Gensim library.\n",
    "\n",
    "Note that some code has been adapted from Nathan Kelber and Ted Lawless's Topic Modelling tutorial: [Latent Dirichlet Allocation (LDA) Topic Modeling\n",
    "](https://github.com/ithaka/tdm-notebooks/blob/master/topic-modeling.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "To begin, we need to import the libraries and download the dataset. Besides some standard libraries, Gensim is a Python library for topic modelling and other natural language processing techniques, and has support for both LDA and NMF, as well as calculating coherence scores.\n",
    "\n",
    "You will also need to download the dataset from Constellate. If you have a dataset that you'd like to perform topic modelling on, you can replace the dataset ID provided with your own dataset.\n",
    "\n",
    "If you are running this tutorial in Constellate, make sure that you have a JSTOR account and link your account to your institution: [Constellate Log-in](https://constellate.org/docs/log-in)\n",
    "\n",
    "If you are running this tutorial through Constellate, please note that Constellate uses an older version of the Gensim library that does not include NMF. To update to the latest version of Gensim, you can run the following code block and then restart the kernel before proceeding. More information is available here: [How to install packages in Constellate](https://constellate.org/docs/install-a-package-in-constellate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code first to update the Gensim library and then restart the kernel (type 0 twice quickly)\n",
    "!pip install gensim -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constellate: use and download of datasets is covered by the Terms & Conditions of Use: https://constellate.org/terms-and-conditions/\n",
      "INFO:numexpr.utils:NumExpr defaulting to 6 threads.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and dataset\n",
    "import constellate\n",
    "import gensim\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"digital scholarship\" from JSTOR or Portico limited to document type(s) article from 1900 - 2023. 2406 documents.\n",
      "INFO:root:File C:\\Users\\Paul K\\data\\d96db0ac-fb15-d85a-2e88-fe9025c7cf5c-sampled-jsonl.jsonl.gz exists. Not re-downloading.\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset\n",
    "\n",
    "# If you have created a dataset with the Constellate Dataset Builder, you can input it here\n",
    "# Otherwise, you may use the dataset supplied with this tutorial\n",
    "# Default is \"digital scholarship\" from JSTOR or Portico limited to document type(s) article from 1900 - 2023 - (2,406 total documents)\n",
    "\n",
    "dataset_id = \"d96db0ac-fb15-d85a-2e88-fe9025c7cf5c\" # Replace with your own dataset ID\n",
    "dataset_file = constellate.get_dataset(dataset_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stoplist\n",
    "Before we proceed with pre-processing tokens, we will want a stoplist. A stoplist is a list of words we will omit from our topic model, because they are too common or provide no semantic value. The NLTK library (Natural Language Toolkit) provides a basic stoplist of very common English words. However, you will likely find words particular to your dataset that show up in most documents, and thus will reduce your topic model's effectiveness. For instance, in our example dataset, because our documents are about Digital Scholarship, we find that most documents will include the word \"information\", so we want to remove it. Topic modelling is an iterative process; it is normal to run your topic model, look at the resultant topics, then go back and extend your stoplist with words that you do not want in your topic model. Another way to extend your stoplist is to calculate the frequency of the most common tokens and find the most common terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import stopword list\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stoplist = [] # You can extend your stoplist with additional words\n",
    "# Example:\n",
    "# new_stoplist = [\"libraries\",\"library\",\"information\"]\n",
    "stop_words.extend(new_stoplist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing tokens\n",
    "Constellate datasets supply unigram tokens directly, which are essentially single word units that can be understood by topic models. This is drastically simpler than, say, scraping websites for text data, which would then need to be converted into unigrams. However, these tokens can still contain punctuation and numbers which do not contain semantic information and will prevent the topic model from reading them properly. Tokens can also contain upper-case characters which need to be converted to lower-case. We will define a function in the next code block that will be used to pre-process unigram tokens as they are extracted from the dataset. We use a Regular Expression to remove all non-alphabetical characters.\n",
    "\n",
    "The script below also includes a lemmatizer from NLTK. The lemmatizer converts inflected forms of words into their lemma, or base forms. We would lemmatize \"worse\" to \"bad\" and \"running\" to \"run\". This way, unigrams with the same lemma are treated the same way in the topic model. If you have a very large dataset that is slow to pre-process, you could consider removing the lemmatizer, however this will decrease the accuracy of the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def process_token(token):\n",
    "    token = token.lower()\n",
    "    token = re.sub('[^a-z]+', '', token)\n",
    "    token = wordnet_lemmatizer.lemmatize(token) # The lemmatizer is very slow, so you may comment this line out with # to speed up the next step\n",
    "    if token in stop_words:\n",
    "        return\n",
    "    if len(token) < 4:\n",
    "        return\n",
    "    if not(token.isalpha()):\n",
    "        return\n",
    "    return token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our process_token function defined, we can proceed with extracting and pre-processing unigram tokens from documents. We retrieve documents with the constellate.dataset_reader function, then iterate through each unigram in each document, creating a list of pre-processed unigram tokens for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import unigram counts from dataset\n",
    "# Datasets in Constellate already have extracted unigrams, which simplifies our process\n",
    "\n",
    "documents = []\n",
    "for document in constellate.dataset_reader(dataset_file):\n",
    "    processed_document = []\n",
    "    unigrams = document.get(\"unigramCount\", {})\n",
    "    for gram, count in unigrams.items():\n",
    "        clean_gram = process_token(gram)\n",
    "        if clean_gram is None:\n",
    "            continue\n",
    "        processed_document += [clean_gram] * count # Add the unigram as many times as it was counted\n",
    "    if len(processed_document) > 0:\n",
    "        documents.append(processed_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[1] # Run this code block to see what a pre-processed document looks like"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two steps will use built-in gensim functions. The first step maps every word to an integer ID in a \"dictionary\". The second step converts each document into a \"bag of words\", a list of each integer ID paired with a count of tokens for that particular document. The result is a document-term matrix, which is necessary for topic modelling. Explaining these functions is beyond the scope of this tutorial, but you can consult the [Gensim API reference](https://radimrehurek.com/gensim/apiref.html) if you would like to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary<0 unique tokens: []>\n",
      "INFO:gensim.corpora.dictionary:built Dictionary<274592 unique tokens: ['ability', 'able', 'academic', 'access', 'accessible']...> from 1500 documents (total 9300191 corpus positions)\n",
      "INFO:gensim.utils:Dictionary lifecycle event {'msg': \"built Dictionary<274592 unique tokens: ['ability', 'able', 'academic', 'access', 'accessible']...> from 1500 documents (total 9300191 corpus positions)\", 'datetime': '2023-04-09T20:18:19.914746', 'gensim': '4.2.0', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary\n",
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create an empty dataframe to store our results. We want to know the model used (LDA or NMF), and the number of topics and passes that we passed as input, as well as the resultant coherence score and computation time. We will retrieve this dataframe later for quantitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Coherence Score</th>\n",
       "      <th>Computation Time</th>\n",
       "      <th>Number of Topics</th>\n",
       "      <th>Passes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model, Coherence Score, Computation Time, Number of Topics, Passes]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantitative results dataset\n",
    "results_df = pd.DataFrame(columns = [\"Model\",\"Coherence Score\",\"Computation Time\",\"Number of Topics\",\"Passes\"])\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence scores\n",
    "If we want to be confident in our topic models, we would like a way to automatically evaluate whether our topics are coherent, and actually represent what a human would consider a coherent topic; that is to say, the words in the topic are relevant and appear to be related. Topic coherence is a measure of semantic similarity between terms in a topic; higher (positive) topic coherence correlates with greater semantic coherence in human evaluation. Gensim provides a function for calculating the coherence score of a topic model. We will integrate this function into a new function, which we will include in our topic model code to calculate and print the topic coherence score.\n",
    "\n",
    "Perplexity is another common metric used to evaluate topic models, used to show how successful a topic model can predict held-out documents from the same corpus. However, topic models that perform well with regards to perplexity do not necessarily have high coherence, by human estimation, nor by calculation (Chang et al., 2009). As well, calculating perplexity is challenging (Wallach et al., 2009), and Gensim supplies \"per-word likelihood bound\" for LDA, a method for estimating perplexity, however they do not provide any function for estimating perplexity for NMF. For these reasons it makes sense to omit perplexity from our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence function\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def coherenceScore(model):\n",
    "    # Compute the coherence score using UMass\n",
    "    # u_mass is measured from -14 to 14, higher is better\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=model,\n",
    "        corpus=bow_corpus,\n",
    "        dictionary=dictionary, \n",
    "        coherence='u_mass'\n",
    "    )\n",
    "\n",
    "    # Compute Coherence Score using UMass\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_score)\n",
    "    return coherence_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The topic models\n",
    "The next two code blocks will produce the topic models. For both LDA and NMF, we need to supply the document-term matrix, the dictionary, the number of topics and the number of passes through the corpus. In theory, more passes will result in more coherent topics.\n",
    "\n",
    "We use the Time Python library to calculate the time it takes for the model to run in seconds. Besides running the model, we want to collect the coherence score and computation time and add them to our Results DataFrame for future evaluation.\n",
    "\n",
    "Optimizing the ideal number of topics and passes is a nontrivial task. More passes will also mean longer computation time. For those with greater comfort in Python, it is possible to iterate over a selection of numbers of topics and passes, and plot the resulting coherence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would like to keep our number of topics and passes constant, so we will set the same values for both algorithms\n",
    "num_topics = 7 # Change this to alter the number of topics\n",
    "passes = 3 # Change this to alter the number of passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.14285714285714285\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.14285714285714285\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (multi-pass) LDA training, 7 topics, 3 passes over the supplied corpus of 1500 documents, updating model once every 1500 documents, evaluating perplexity every 1500 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-13.205 per-word bound, 9439.8 perplexity estimate based on a held-out corpus of 1500 documents with 9300191 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1500/1500\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.143): 0.007*\"library\" + 0.006*\"information\" + 0.006*\"research\" + 0.006*\"university\" + 0.005*\"digital\" + 0.005*\"journal\" + 0.005*\"data\" + 0.004*\"study\" + 0.003*\"work\" + 0.003*\"student\"\n",
      "INFO:gensim.models.ldamodel:topic #5 (0.143): 0.009*\"library\" + 0.009*\"information\" + 0.007*\"study\" + 0.006*\"data\" + 0.005*\"research\" + 0.005*\"university\" + 0.004*\"digital\" + 0.004*\"journal\" + 0.004*\"also\" + 0.004*\"student\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.143): 0.009*\"library\" + 0.008*\"research\" + 0.006*\"study\" + 0.005*\"digital\" + 0.004*\"journal\" + 0.004*\"information\" + 0.004*\"student\" + 0.003*\"data\" + 0.003*\"also\" + 0.003*\"social\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.143): 0.007*\"library\" + 0.006*\"information\" + 0.006*\"research\" + 0.006*\"digital\" + 0.005*\"university\" + 0.004*\"study\" + 0.004*\"data\" + 0.004*\"also\" + 0.003*\"student\" + 0.003*\"work\"\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.143): 0.007*\"digital\" + 0.006*\"information\" + 0.005*\"library\" + 0.005*\"study\" + 0.005*\"research\" + 0.004*\"university\" + 0.004*\"data\" + 0.004*\"also\" + 0.003*\"student\" + 0.003*\"project\"\n",
      "INFO:gensim.models.ldamodel:topic diff=3.657995, rho=1.000000\n",
      "INFO:gensim.models.ldamodel:-9.016 per-word bound, 517.7 perplexity estimate based on a held-out corpus of 1500 documents with 9300191 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 1, at document #1500/1500\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.143): 0.006*\"research\" + 0.006*\"university\" + 0.006*\"information\" + 0.005*\"digital\" + 0.005*\"univ\" + 0.005*\"library\" + 0.005*\"journal\" + 0.004*\"study\" + 0.004*\"knowledge\" + 0.004*\"literature\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.143): 0.005*\"library\" + 0.005*\"university\" + 0.005*\"digital\" + 0.005*\"information\" + 0.005*\"research\" + 0.004*\"study\" + 0.004*\"also\" + 0.003*\"data\" + 0.003*\"work\" + 0.002*\"text\"\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.143): 0.008*\"digital\" + 0.005*\"history\" + 0.004*\"information\" + 0.004*\"study\" + 0.004*\"also\" + 0.004*\"university\" + 0.004*\"project\" + 0.004*\"work\" + 0.004*\"research\" + 0.003*\"library\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.143): 0.009*\"univ\" + 0.006*\"library\" + 0.005*\"research\" + 0.005*\"student\" + 0.005*\"social\" + 0.004*\"study\" + 0.004*\"information\" + 0.004*\"university\" + 0.003*\"work\" + 0.003*\"digital\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.143): 0.012*\"library\" + 0.008*\"research\" + 0.006*\"digital\" + 0.006*\"study\" + 0.004*\"student\" + 0.004*\"journal\" + 0.004*\"service\" + 0.004*\"information\" + 0.003*\"also\" + 0.003*\"university\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.998773, rho=0.577350\n",
      "INFO:gensim.models.ldamodel:-8.857 per-word bound, 463.8 perplexity estimate based on a held-out corpus of 1500 documents with 9300191 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 2, at document #1500/1500\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.143): 0.006*\"university\" + 0.005*\"research\" + 0.005*\"digital\" + 0.005*\"journal\" + 0.005*\"knowledge\" + 0.005*\"information\" + 0.004*\"literature\" + 0.004*\"study\" + 0.004*\"univ\" + 0.004*\"library\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.143): 0.016*\"univ\" + 0.005*\"study\" + 0.004*\"state\" + 0.004*\"student\" + 0.004*\"program\" + 0.004*\"library\" + 0.004*\"research\" + 0.004*\"american\" + 0.004*\"literature\" + 0.004*\"social\"\n",
      "INFO:gensim.models.ldamodel:topic #5 (0.143): 0.013*\"library\" + 0.013*\"information\" + 0.008*\"research\" + 0.008*\"data\" + 0.007*\"study\" + 0.005*\"university\" + 0.005*\"student\" + 0.005*\"journal\" + 0.004*\"digital\" + 0.004*\"user\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.143): 0.014*\"library\" + 0.007*\"research\" + 0.007*\"digital\" + 0.005*\"student\" + 0.005*\"study\" + 0.004*\"service\" + 0.004*\"university\" + 0.004*\"journal\" + 0.004*\"also\" + 0.003*\"available\"\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.143): 0.009*\"digital\" + 0.006*\"history\" + 0.005*\"work\" + 0.004*\"also\" + 0.004*\"project\" + 0.004*\"study\" + 0.004*\"university\" + 0.004*\"book\" + 0.003*\"research\" + 0.003*\"text\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.745992, rho=0.500000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=274592, num_topics=7, decay=0.5, chunksize=2000> in 58.28s', 'datetime': '2023-04-09T20:19:32.712610', 'gensim': '4.2.0', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "\n",
      "Coherence Score:  -0.34202979262825567\n",
      "58.49924826622009\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "# Train the LDA model\n",
    "start = time.time()\n",
    "\n",
    "LDAmodel = gensim.models.LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "coherence = coherenceScore(LDAmodel)\n",
    "print(end-start)\n",
    "results_df.loc[len(results_df.index)] = [\"LDA\",coherence,end-start,num_topics,passes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.nmf:running NMF training, 7 topics, 3 passes over the supplied corpus of 1500 documents, evaluating L2 norm every 1500 documents\n",
      "INFO:gensim.models.nmf:PROGRESS: pass 0, at document #1500/1500\n",
      "INFO:gensim.models.nmf:L2 norm: 27637.779991499665\n",
      "INFO:gensim.models.nmf:topic #0 (0.000): 0.000*\"alonainy\" + 0.000*\"januaryjuly\" + 0.000*\"callon\" + 0.000*\"libertas\" + 0.000*\"rotten\" + 0.000*\"ared\" + 0.000*\"chemosphere\" + 0.000*\"menstruation\" + 0.000*\"sessanta\" + 0.000*\"embeddings\"\n",
      "INFO:gensim.models.nmf:topic #1 (0.000): 0.000*\"softbook\" + 0.000*\"etezadiamoli\" + 0.000*\"canonical\" + 0.000*\"httpthatcamporg\" + 0.000*\"wwwmuseumsandthewebcommwpapersex\" + 0.000*\"indicted\" + 0.000*\"hrlindgrenpipl\" + 0.000*\"archaeoastronomers\" + 0.000*\"unavailable\" + 0.000*\"nowviskieorgnwherecreditisdue\"\n",
      "INFO:gensim.models.nmf:topic #4 (0.000): 0.000*\"ispirato\" + 0.000*\"tawiah\" + 0.000*\"domaingeneral\" + 0.000*\"onlyeasily\" + 0.000*\"advocacy\" + 0.000*\"courier\" + 0.000*\"universitaa\" + 0.000*\"sioning\" + 0.000*\"hauberg\" + 0.000*\"roadtoxanadu\"\n",
      "INFO:gensim.models.nmf:topic #5 (0.000): 0.000*\"kirchhoff\" + 0.000*\"overdispersion\" + 0.000*\"mangm\" + 0.000*\"deadliness\" + 0.000*\"avverbiale\" + 0.000*\"wwwbersincomblogpostaspxid\" + 0.000*\"futurefarmers\" + 0.000*\"spaceor\" + 0.000*\"torkzadeh\" + 0.000*\"comunicao\"\n",
      "INFO:gensim.models.nmf:topic #6 (0.000): 0.000*\"biodefense\" + 0.000*\"highundergradu\" + 0.000*\"pearsons\" + 0.000*\"mcculloh\" + 0.000*\"futurelibrarypdf\" + 0.000*\"messina\" + 0.000*\"plugsreceptacles\" + 0.000*\"levi\" + 0.000*\"observees\" + 0.000*\"aggravating\"\n",
      "INFO:gensim.models.nmf:W error: -169569040.26076168\n",
      "INFO:gensim.models.nmf:PROGRESS: pass 1, at document #1500/1500\n",
      "INFO:gensim.models.nmf:L2 norm: 15346.034678769767\n",
      "INFO:gensim.models.nmf:topic #3 (0.372): 0.021*\"univ\" + 0.008*\"literature\" + 0.005*\"study\" + 0.005*\"state\" + 0.005*\"program\" + 0.004*\"american\" + 0.004*\"paper\" + 0.004*\"presiding\" + 0.004*\"language\" + 0.003*\"university\"\n",
      "INFO:gensim.models.nmf:topic #2 (0.469): 0.012*\"univ\" + 0.012*\"information\" + 0.007*\"social\" + 0.006*\"literature\" + 0.006*\"study\" + 0.004*\"research\" + 0.004*\"state\" + 0.004*\"science\" + 0.004*\"library\" + 0.004*\"journal\"\n",
      "INFO:gensim.models.nmf:topic #6 (0.666): 0.025*\"information\" + 0.016*\"library\" + 0.009*\"research\" + 0.009*\"knowledge\" + 0.008*\"student\" + 0.007*\"data\" + 0.007*\"journal\" + 0.006*\"digital\" + 0.005*\"literacy\" + 0.005*\"study\"\n",
      "INFO:gensim.models.nmf:topic #0 (0.679): 0.024*\"patient\" + 0.013*\"pediatric\" + 0.012*\"cancer\" + 0.012*\"univ\" + 0.012*\"child\" + 0.010*\"hospital\" + 0.010*\"oncology\" + 0.008*\"treatment\" + 0.008*\"year\" + 0.008*\"tumor\"\n",
      "INFO:gensim.models.nmf:topic #1 (0.766): 0.028*\"patient\" + 0.015*\"pediatric\" + 0.014*\"cancer\" + 0.014*\"child\" + 0.012*\"hospital\" + 0.012*\"oncology\" + 0.009*\"treatment\" + 0.009*\"tumor\" + 0.009*\"year\" + 0.007*\"result\"\n",
      "INFO:gensim.models.nmf:W error: -225358770.143253\n",
      "INFO:gensim.models.nmf:PROGRESS: pass 2, at document #1500/1500\n",
      "INFO:gensim.models.nmf:L2 norm: 14066.049837065364\n",
      "INFO:gensim.models.nmf:topic #5 (0.284): 0.012*\"research\" + 0.012*\"data\" + 0.008*\"study\" + 0.008*\"university\" + 0.005*\"management\" + 0.004*\"journal\" + 0.004*\"knowledge\" + 0.004*\"science\" + 0.004*\"system\" + 0.004*\"also\"\n",
      "INFO:gensim.models.nmf:topic #2 (0.317): 0.012*\"information\" + 0.008*\"social\" + 0.006*\"study\" + 0.005*\"univ\" + 0.005*\"literature\" + 0.004*\"work\" + 0.004*\"journal\" + 0.004*\"digital\" + 0.004*\"medium\" + 0.004*\"knowledge\"\n",
      "INFO:gensim.models.nmf:topic #0 (0.632): 0.023*\"patient\" + 0.013*\"univ\" + 0.012*\"pediatric\" + 0.012*\"cancer\" + 0.012*\"child\" + 0.010*\"hospital\" + 0.010*\"oncology\" + 0.008*\"treatment\" + 0.008*\"year\" + 0.007*\"tumor\"\n",
      "INFO:gensim.models.nmf:topic #6 (0.726): 0.030*\"information\" + 0.018*\"library\" + 0.009*\"research\" + 0.007*\"knowledge\" + 0.007*\"data\" + 0.007*\"student\" + 0.006*\"study\" + 0.006*\"user\" + 0.006*\"literacy\" + 0.006*\"journal\"\n",
      "INFO:gensim.models.nmf:topic #1 (0.852): 0.026*\"patient\" + 0.014*\"pediatric\" + 0.013*\"cancer\" + 0.013*\"child\" + 0.011*\"hospital\" + 0.011*\"oncology\" + 0.009*\"treatment\" + 0.008*\"tumor\" + 0.008*\"year\" + 0.007*\"result\"\n",
      "INFO:gensim.models.nmf:W error: -245851358.51953948\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "\n",
      "Coherence Score:  -0.8475157127520568\n",
      "30.322423696517944\n"
     ]
    }
   ],
   "source": [
    "# NMF\n",
    "# Train the NMF model\n",
    "start = time.time()\n",
    "\n",
    "NMFmodel = gensim.models.nmf.Nmf(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "coherence = coherenceScore(NMFmodel)\n",
    "print(end-start)\n",
    "results_df.loc[len(results_df.index)] = [\"NMF\",coherence,end-start,num_topics,passes]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Analysis\n",
    "Having run both LDA and NMF models, we would like to see the results. In general, LDA models will have higher coherence scores, whereas NMF models will have faster computation times (Egger and Yu, 2022). However, there is no one-size-fits-all option for topic modelling, and optimizing your topic model will be dependent on your particular dataset. More importantly, we must consider these quantitative results in the context of the resulant topics, which is covered in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Coherence Score</th>\n",
       "      <th>Computation Time</th>\n",
       "      <th>Number of Topics</th>\n",
       "      <th>Passes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDA</td>\n",
       "      <td>-0.342030</td>\n",
       "      <td>58.499248</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NMF</td>\n",
       "      <td>-0.847516</td>\n",
       "      <td>30.322424</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Coherence Score  Computation Time Number of Topics Passes\n",
       "0   LDA        -0.342030         58.499248                7      3\n",
       "1   NMF        -0.847516         30.322424                7      3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this code block to compare results of all topic models performed\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Analysis\n",
    "Besides coherence scores and computation times, we will also need to qualitatively evaluate the resultant topics of our models. Again, this is This qualitative analysis is best suited for a domain expert familiar with the corpus. We are looking for topics that have an internal semantic coherence. However, we would also like to see that our topics are suitably dissimilar, without significant overlap. Topic modelling is an iterative proces. If you find that there is too much overlap in your topics, consider adding additional terms to your stoplist, or increasing the number of passes of your topic models. If you are satisfied with your topics, consider finding the documents which best match your topics and reading them to better learn what your topics represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                               Topics\n",
      "0                    [digital, history, work, also, project, study, university, book, research, text]\n",
      "1                [university, library, study, digital, research, information, also, text, work, data]\n",
      "2      [research, journal, data, study, library, information, university, language, digital, patient]\n",
      "3             [univ, study, state, student, program, library, research, american, literature, social]\n",
      "4  [university, research, digital, journal, knowledge, information, literature, study, univ, library]\n",
      "5          [library, information, research, data, study, university, student, journal, digital, user]\n",
      "6         [library, research, digital, student, study, service, university, journal, also, available]\n",
      "                                                                                              Topics\n",
      "0        [patient, pediatric, cancer, child, hospital, oncology, treatment, year, tumor, department]\n",
      "1            [patient, pediatric, cancer, child, hospital, oncology, treatment, tumor, year, result]\n",
      "2          [information, social, study, work, digital, journal, knowledge, literature, medium, also]\n",
      "3          [univ, literature, presiding, state, program, arranged, coll, session, american, chicago]\n",
      "4  [library, student, service, university, research, learning, librarian, academic, study, resource]\n",
      "5         [research, data, study, university, management, knowledge, journal, system, science, also]\n",
      "6         [information, library, research, data, study, literacy, student, user, knowledge, journal]\n"
     ]
    }
   ],
   "source": [
    "# Print topics of two last models\n",
    "# need to adapt this code to match the rest of the tutorial\n",
    "# Retrieve the topics for k = 5\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "lda_topics = []\n",
    "for i in range(num_topics):\n",
    "    topic = LDAmodel.show_topic(i,10)\n",
    "    lda_topics.append([x for x,y in topic])\n",
    "lda_topics_df = pd.DataFrame(lda_topics,index=range(num_topics))\n",
    "lda_topics_df['Topics'] = lda_topics_df.values.tolist()\n",
    "lda_topics_df = pd.DataFrame(lda_topics_df['Topics'],index=range(num_topics))\n",
    "print(lda_topics_df)\n",
    "nmf_topics = []\n",
    "for i in range(num_topics):\n",
    "    topic = NMFmodel.show_topic(i,10)\n",
    "    nmf_topics.append([x for x,y in topic])\n",
    "nmf_topics_df = pd.DataFrame(nmf_topics,index=range(num_topics))\n",
    "nmf_topics_df['Topics'] = nmf_topics_df.values.tolist()\n",
    "nmf_topics_df = pd.DataFrame(nmf_topics_df['Topics'],index=range(num_topics))\n",
    "print(nmf_topics_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "Text analytic algorithms such as LDA and NMF are often evaluated by quantitative metrics such as perplexity and coherence. However, in the age of Large Language Models such as OpenAI's GPT-4, there is also a need to evaluate these algorithms by time and energy usage (Strubell et al., 2019). Topics must also be evaluated qualitatively by a domain expert, as topic models with poorly tuned parameters can have flaws that are obvious to experts but not novices (Mimno et al., 2011). It is my hope that this tutorial has inspired you to think about these algorithms with respect to computational costs and qualitative analysis.\n",
    "\n",
    "There are other algorithms that can be used for topic modelling, including Biterm Topic Modelling and Correlation Explanation (or CorEx). CorEx differs from other topic modelling algorithms in that it permits the use of \"semi-supervised modelling\", allowing scholars to provide anchor terms for topics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "Chang, J., Gerrish, S., Wang, C., Boyd-graber, J., & Blei, D. (2009). Reading Tea Leaves: How Humans Interpret Topic Models. Advances in Neural Information Processing Systems, 22. https://papers.nips.cc/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html\n",
    "\n",
    "Egger, R., & Yu, J. (2022). A Topic Modeling Comparison Between LDA, NMF, Top2Vec, and BERTopic to Demystify Twitter Posts. Frontiers in Sociology, 7. https://www.frontiersin.org/articles/10.3389/fsoc.2022.886498\n",
    "\n",
    "Mimno, D., Wallach, H., Talley, E., Leenders, M., & McCallum, A. (2011). Optimizing Semantic Coherence in Topic Models. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 262–272.\n",
    "\n",
    "Strubell, E., Ganesh, A., & McCallum, A. (2019). Energy and Policy Considerations for Deep Learning in NLP. ArXiv:1906.02243 [Cs]. http://arxiv.org/abs/1906.02243\n",
    "\n",
    "Wallach, H. M., Murray, I., Salakhutdinov, R., & Mimno, D. (2009). Evaluation methods for topic models. Proceedings of the 26th Annual International Conference on Machine Learning, 1105–1112. https://doi.org/10.1145/1553374.1553515\n",
    "\n",
    "## Resources and Recommended Reading\n",
    "- Constellate tutorials\n",
    "\n",
    "- BTM\n",
    "\n",
    "- CorEx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
